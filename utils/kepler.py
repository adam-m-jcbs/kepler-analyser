# Routines and classes for reading KEPLER models (dump files)
# Each dump file contains 1 model.

import bec_utils as bec
import numpy as n
from numpy import rec
from glob import glob
import os, time
import struct
from scipy import signal
import kepler_parameters as kep_par
import logging

def load_model(filename, byteorder='>'):
    """
    Load a model from a single dump file without indexing the entire directory.
    """
    f = bec.FortranFile(filename, byteorder)
    m = KeplerModel(f)
    return m

def to_castro(filename):
    """
    Convenience routine for exporting kepler dumps with filename to an ascii file that CASTRO can read
    """
    m = load_model(filename)
    m.to_ascii(fields=['gridpoints', 'dm', 'r', 'u', 'ro', 't', 'pn', 'en', 'w'], approx=True,
               comment='zone number, zone mass, radius, velocity, density, temperature, pressure, internal energy, angular velocity, isotopic mass fractions')

def isotopes2ascii(model, zone, filename=None, min_fraction=-1):
    """
    Create an ascii file with the isotopic mass fractions of given zone in given model.
    If no filename is specified, the name of the model dump file is appended with _<zone>.txt.
    Isotopes with a mass fraction below min_fraction are omitted.
    """
    if filename==None:
        filename = '%s_%i.txt'%(model.filename, zone)
    f = open(filename, 'w')
    
    # Header
    f.write('# %s UTC -- created by %s on %s from file %s -- zone %i\n'%(time.asctime(time.gmtime()),
                                                                            os.getenv('USER'),
                                                                            os.getenv('HOSTNAME'),
                                                                            model.filename,
                                                                            zone))
    f.write('# T = %.7E K\n'%(model['t'][zone], ))
    f.write('# ro = %.7E  g/cm3\n'%(model['ro'][zone], ))
    f.write('#\n')
    f.write('# Columns:\n')
    f.write('# 1. Isotope name\n')
    f.write('# 2. Z\n')
    f.write('# 3. A\n')
    f.write('# 4. Mass fraction\n')
    f.write('#\n')
    
    # Isotopic mass fractions
    Z = model.Z()
    A = model.A()
    names = model.isotope_names()
    ind = n.argsort(Z) # Sort isotopes by Z, A
    Z = Z[ind]
    A = A[ind]
    names = names[ind]
    ind = n.argsort(A, kind='mergesort')
    Z = Z[ind]
    A = A[ind]
    names = names[ind]
    for name, z, a in zip(names, Z, A):
        mass_fraction = model[name][zone]
        if mass_fraction >=min_fraction:
            f.write('%5s %3i %3i %25.17E\n'%(name, z, a, mass_fraction))
    f.close()

def debug(enable=True):
    """
    Set debug output for KeplerModel
    """
    if enable:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    
class DumpFiles(bec.BinFile):
    """
    A fake file representation of a series of model dumps generated by
    Kepler.
    """

    def __init__(self, filename, byteorder='>'):
        """
        Initialize a new DumpFiles. Dump files will be searched using
        filename = path/problem_name.
        """
        bec.BinFile.__init__(self, filename, byteorder)
        
    def open(self, filename):
        """
        Open the file (fake)
        """
        self.filename = filename
    
    def build_index(self):
        """Generate an index of the dump files."""
        # Search files
        files = glob(self.filename+'#[0-9]*')
        files = [file for file in files if not file.endswith('.txt')] # Exclude ascii dumps
        
        # Order
        number = n.array([int(file.split('#')[1]) for file in files])
        self.index = [files[i] for i in number.argsort()]

        # Add latest files
        filename = self.filename + 'z1'
        if os.path.exists(filename):
            self.index.append(filename)
        filename = self.filename + 'z'
        if os.path.exists(filename):
            self.index.append(filename)
    
    def read_a_model(self, index):
        """Read requested model"""
        f = bec.FortranFile(self.index[index], self.byteorder)
        m = KeplerModel(f)
        
        return m

class KeplerModel(bec.Model):
    """
    Contains all information of one Kepler model.
    """
    
    def __init__(self, fortran_file, byteorder='>'):
        """
        Create a new KeplerModel using data from given
        FortranFile.
        debug: whether to print debug info to stdout
        """
        nburn = 8192 # Defined in nburncom
        
        logger = logging.getLogger('KeplerModel.init')
        self.byteorder = byteorder
        self.filename = fortran_file.filename
        logger.debug('Loading KeplerModel from %s with byte order %s'%(self.filename, self.byteorder))
        
        # header
        data = fortran_file.read_a_record(0)
        
        # See list in kepcom starting with hdum (=nvers,ncyc; see dumpio.f restart())
        # Note that idumhead is a placeholder in kepler for future header parameters
        names = ['nvers', 'ncyc', 'lenhead0', 'jmz0', 'jmzb0', 'nburn0', 'iratioz0', 'nvar0',
                 'nheadz0', 'nhead0', 'nparmz0', 'nparm0', 'nqparmz0', 'nqparm0',
                 'nniz0', 'nhiz0', 'nitz0', 'nnizb0', 'nhizb0', 'nitzb0',
                 'nreacz0', 'ndtz0', 'ndt0', 'npistz0', 'nyez0', 'nsubz0', 'nsub0',
                 'nzonei0', 'nzonec0', 'nzoneb0', 'nsmall0', 'nsmallc0',
                 'imaxa0', 'imaxb0', 'nreac0', 'jmsave', 'lencom0', 'lencomc0',
                 'nedtcom0', 'ndatqz0', 'ngridz0', 'nylibz0', 'nyoffst0',
                 'lenshed0', 'lenqhed0', 'nzedz0', 'ncsavdz0']
        fmt = len(names)*['i4']
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
        self.extract(r)
        if self.nvers < 170000: # This is in dumpio.restart: do we need this?
            self.nqparmz0 = self.nqparm0
        logger.debug('Read header: model version %i, cycle %i'%(self.nvers, self.ncyc))
        
        # p parameters
        data = fortran_file.read_a_record(1)
        names = ['pdum']
        fmt = 'f8'
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=(self.nparmz0+1))
        self.extract(r)
        names = ['npdum']
        fmt = 'i4'
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=2*(self.nparmz0+1))
        self.extract(r)
        self.npdum = self.npdum[::2] # Only 4 out of 8 bits used. Possible endianness problem!
        self.p = kep_par.ParameterMap(self.npdum, self.pdum, kep_par.p)
        logger.debug('Read p parameters')
        if self.nvers==0:
            old_version = self.p.get(66, 1)
            self.version = int(10000.0*old_version + 0.1) # Version convertion following dumpio.restart
            logger.debug('Converted version number from {} to {}'.format(old_version, self.version))
        else:
            self.version = self.nvers
        version = self.version
        
        # q parameters
        data = fortran_file.read_a_record(2)
        names = ['qdum']
        fmt = 'f8'
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=(self.nqparmz0+1))
        self.extract(r)
        names = ['nqdum']
        fmt = 'i4'
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=2*(self.nqparmz0+1))
        self.extract(r)
        self.nqdum = self.nqdum[::2] # Only 4 out of 8 bits used. Possible endianness problem!
        self.q = kep_par.ParameterMap(self.nqdum, self.qdum, kep_par.q)
        logger.debug('Read q parameters')

        nsetparm = self.p['setparm'] # From dumpio.restart: needed here?
        if nsetparm>159950:
            nsmallc1 = self.nsmallc0 - (1 + self.jmsave) + (nburn - self.imaxb0)
            nsmall1 = self.nsmall0 + 4*(nburn - self.imaxb0)
        else:
            nsmallc1 = self.nsmallc0
            nsmall1 = self.nsmall0
        
        # ion, burn ion, piston, wind(b) data
        # aion, zion: A,Z for isotopes. H1 is there twice and should be summed.
        # numi: number of isotopes for each network
        data = fortran_file.read_a_record(3)
        names = ['tpist', 'rpist', 'yemass', 'yeq0',
                 'aion', 'zion', 'numi', 'ionn']
        fmts = ['f8', 'f8', 'f8', 'f8',
                'f8', 'f8', 'i4', 'i4']
        shapes = [(self.npistz0), (self.npistz0), (self.nyez0), (self.nyez0),
                  (self.nitz0), (self.nitz0), 2*(self.nniz0), (2*self.nniz0, self.nhiz0)]
        pos = 8 # Offset for sdum place holder in kepler
        for name, shape, fmt in zip(names, shapes, fmts):
            size = n.prod(shape)*8
            r = rec.fromstring(data[pos:pos+size], formats=fmt, names=[name],
                               byteorder=byteorder, shape=shape)
            self.extract(r)
            pos = pos + size
        self.numi = self.numi[::2] # Only 4 out of 8 bits used. Possible endianness problem!
        self.ionn = self.ionn[::2,:] # Only 4 out of 8 bits used. Possible endianness problem!
        self.ionn = self.ionn.transpose() # Fortran stores column, row
        # TODO: there is more data is sdum
        logger.debug('Read small arrays: piston, ye, ion')
        
        # cdum: only read ions, ionsb, iconv
        data = fortran_file.read_a_record(4)
        pos = 6*8 + 4*16
        names = ['ions']
        fmt = 'a8'
        size = 8*self.nitz0
        r = rec.fromstring(data[pos:pos+size], formats=fmt, names=names, byteorder=byteorder, shape=(self.nitz0))
        self.extract(r)
        
        pos += size
        if self.version>151750:
            pos += 8*10
        if self.version<159950:
            pos += 8*self.nitzb0
        else:
            pos += 8*self.imaxb0
        
        names = ['iconv']
        size = 8*(self.jmsave+1)
        r = rec.fromstring(data[pos:pos+size], formats=fmt, names=names, byteorder=byteorder, shape=(self.jmsave+1))
        self.extract(r)
        logger.debug('Read character arrays: ions, ionsb, iconv')
        
        # stellar structure, rotation
        names = ['ym', 'rn', 'rd', 'un', 'xln', 'qln', 'qld', 'difi', 'znetnum', 'Xm', 'dn', 'tn', 'td',
                 'en', 'pn', 'zn', 'etan', 'sn', 'snn', 'abar', 'zbar', 'xkn', 'xnei', 'stot', 'angj',
                 'angdg', 'angd1', 'angd2', 'angd3', 'angd4', 'angd5', 'dsold', 'tsold']
        for i, name in enumerate(names):
            if name=='znetnum':
                fmt = 'i4'
                shape = (self.jmsave+1)*2
            else:
                fmt = 'f8'
                shape = self.jmsave+1
            data = fortran_file.read_a_record(5 + i)
            r = rec.fromstring(data, formats=fmt, names=[name], byteorder=byteorder, shape=(shape))
            # Skip last point, which is bogus. The first point only meaningfull for on-grid properties.
            self.extract(r, max=-1)
        self.netnum = self.znetnum[::2] # Only 4 out of 8 bits used. Possible endianness problem!
        last = 5 + i
        logger.debug('Read zonal arrays: stellar structure, rotation, etc.')
        
        # ppn
        jm = self.nqdum[2]
        imax=self.nqdum[14]
        
        data = fortran_file.read_a_record(last + 1)
        last = last + 1
        names = ['ppn']
        fmt = '(%i,%i)f8'%(jm, imax)
        r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
        self.extract(r)
        self.ppn = self.ppn.transpose()
        logger.debug('Read ppn')
        
        # magnetic field
        magnet = self.npdum[423]
        names = ['bfvisc', 'bfdiff']
        if magnet != 0 and version>=161150:
            data = fortran_file.read_a_record(last + 1)
            last = last + 1
            fmt = '%if8,%if8'%(jm, jm)
            r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
            self.extract(r)
            for name in names:
                # Add 0 inner grid point to comply with other off-grid properties.
                setattr(self, name, n.concatenate([[0], getattr(self, name)]))
            logger.debug('Read magnetic field data I')
        else:
            for name in names:
                setattr(self, name, n.zeros_like(self.ym))
            logger.debug('No magnetic field data I')
        
        names = ['bfbr', 'bfbt']
        if magnet != 0 and version>=162650:
            data = fortran_file.read_a_record(last + 1)
            last = last + 1
            fmt = '%if8,%if8'%(jm, jm)
            r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
            self.extract(r)
            for name in names:
                setattr(self, name, n.concatenate([[0], getattr(self, name)]))
            logger.debug('Read magnetic field data II')
        else:
            for name in names:
                setattr(self, name, n.zeros_like(self.ym))
            logger.debug('No magnetic field data II')
        
        names = ['bfviscef', 'bfdiffef']
        if magnet != 0 and version>=162150:
            data = fortran_file.read_a_record(last + 1)
            last = last + 1
            fmt = '%if8,%if8'%(jm, jm)
            r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
            self.extract(r)
            for name in names:
                setattr(self, name, n.concatenate([[0], getattr(self, name)]))
            logger.debug('Read magnetic field data III')
        else:
            for name in names:
                setattr(self, name, n.zeros_like(self.ym))
            logger.debug('No magnetic field data III')
        
        names = ['angdgeff', 'difieff']
        if version>=162150:
            data = fortran_file.read_a_record(last + 1)
            last = last + 1
            fmt = '%if8,%if8'%(jm, jm)
            r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
            self.extract(r)
            for name in names:
                setattr(self, name, n.concatenate([[0], getattr(self, name)]))
            logger.debug('Read rotation data')
        else:
            for name in names:
                setattr(self, name, n.zeros_like(self.ym))
        
        # Flame data: skipped
        if version>=161350:
            sharp1 = self.pdum[282]
            if sharp1>0:
                last = last + 1
                logger.warning('Skipping flame data')
        
        if version>=169250:
            # WIMP data: skipped
            wimp = self.pdum[496]
            if version>=167550 and wimp>0.0:
                last = last + 3
                logger.warning('Skipping WIMP data')
            
            # Advection energy deposition
            if version>=168150:
                data = fortran_file.read_a_record(last + 1)
                last = last + 1
                name = 'sadv'
                fmt = '%if8'%jm
                r = rec.fromstring(data, formats=fmt, names=[name], byteorder=byteorder, shape=1)
                self.extract(r)
                logger.debug('Read advection data')
            
            # UUID and log data (skipping log)
            if version>=168450:
                data = fortran_file.read_a_record(last + 1)
                last += 1
                names = ['uuidrun', 'uuidcycle', 'uuiddump', 'uuidprev', 'uuidprog']
                fmt = len(names)*',a16'
                fmt = fmt[1:]
                r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                self.extract(r)
                logger.debug('Read uuid data')
                
                last = last + 1
                logger.debug('Skipping log data')
            
            # Rotation specific viscous energy dissipation rate
            if version>=169750:
                data = fortran_file.read_a_record(last + 1)
                last = last + 1
                name = 'sv'
                fmt = '%if8'%jm
                r = rec.fromstring(data, formats=fmt, names=[name], byteorder=byteorder, shape=1)
                self.extract(r)
                logger.debug('Read viscous energy dissipation data')
            
            if version>=170002:
                data = fortran_file.read_a_record(last + 1)
                last = last + 1
                noparm = struct.unpack('i', data[:4])[0] # Number of parameters
                data = data[4:]
                
                names = ['nameoprm', 'iotype', 'odum']
                if noparm>0:
                    fmt = '%if8,%if8,%if8'%(noparm, noparm, noparm)
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                logger.debug('Read %i user-defined parameters'%noparm)
        
        elif version>=167550:
            pass
            # The burn data should still load fine, but the above will not be loaded.
            # This is because of a kepler version that had the order mixed up.
        
        # Burn network
        imaxb = self.nqdum[53]
        nsaveb = self.npdum[270]
        jmsave1 = self.jmsave + 1
        nbdum1 = jmsave1
        if imaxb>0 and self.jmzb0 > 1 and nsaveb > 0:
            if nsaveb != 10:
                logger.error("Error loading burn network data: nsaveb = %i"%nsaveb)
            
            names = ['netnumb', 'zlimnuc', 'timen', 'dtimen', 'dnold',
                     'tnold', 'ymb', 'sburn', 'etab', 'pbuf']
            for name in names:
                print 'Burn', name, last+1, len(fortran_file.index)
                data = fortran_file.read_a_record(last + 1)
                last = last + 1
                if name=='netnumb':
                    fmt = '%ii4,'%nbdum1
                else:
                    fmt = '%if8,'%nbdum1
                r = rec.fromstring(data, formats=fmt, names=[name], byteorder=byteorder, shape=1)
                self.extract(r)
                #print getattr(self, name)
            
            nppnb0=jm*imaxb
            names = ['ppnb']
            data = fortran_file.read_a_record(last + 1)
            last = last + 1
            fmt = '%if8'%(nppnb0)
            r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
            self.extract(r)
            self.ppnb = self.ppnb.reshape([jm, imaxb])
            self.ppnb = self.ppnb.transpose()
            
            irecb = self.npdum[418]
            if irecb==1: # Have burn network isotopes been stored?
                if version>=160395 and version<160850:
                    print "Reading additional burn data for these versions not implemented yet!"
                elif version>=160850:
                    names = ['nbmax'] # Number of isotopes 
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = 'i4'
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    nbmax = self.nbmax
                    
                    names = ['nabmax'] # Isotope mass number A
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%ii4'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    
                    names = ['nzbmax'] # Isotope proton number Z
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%ii4'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    
                    names = ['nibmax'] # Map isotopes to names in ionbmax
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%ii4'%imaxb # This must really be imaxb. In some cases imaxb!=nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    self.data = data
                    
                    names = ['ionbmax'] # Names of isotopes
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%iS8'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    
                    names = ['burnamax'] # Maximum abundance at any grid point
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%if8'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    
                    names = ['burnmmax'] # Mass coordinate of max abundance
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%if8'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    
                    names = ['ibcmax']
                    data = fortran_file.read_a_record(last + 1)
                    last += 1
                    fmt = '%ii4'%nbmax
                    r = rec.fromstring(data, formats=fmt, names=names, byteorder=byteorder, shape=1)
                    self.extract(r)
                    logger.debug('Read BURN data')
        else:
            self.nibmax = []
        
        self.time = self.pdum[2]
        self.dtn = self.pdum[1]
        self.model = self.nqdum[4]
        self.gms = self.pdum[61]/bec.xmsun
        self.n = len(self.Xm)
        self.jm = jm
        self.al = self.angj*self.Xm
        synonyms = {'Xm': 'dm',
                    'rn': 'r',
                    'un': 'u',
                    'dn': 'ro',
                    'en': 'e',
                    'xln': 'sl',
                    #'pn': 'p', # Name conflict with parameter array p
                    'tn': 't',
                    'xkn': 'cap',
                    'angdg': 'dg',
                    'angd1': 'd1',
                    'angd2': 'd2',
                    'angd3': 'd3',
                    'angd4': 'd4',
                    'angd5': 'd5',
                    'angj': 'alspec',
                    'difi': 'diff',
                    }
        for k,v in synonyms.items():
            setattr(self, v, getattr(self, k))
        
        self.isotope_mapping = self.create_isotope_mapping()
        self.ion_mapping = self.create_ion_mapping()
        logger.debug('Data loaded successfully.')
    
    def create_isotope_mapping(self):
        """
        Create a mapping between isotope number and name.
        Used my get_massfraction().
        """
        mapping = {}
        for i,ind in enumerate(self.nibmax):
            mapping[self.ionbmax[ind-1]] = i
        
        return mapping
    
    def create_ion_mapping(self):
        """
        Create a mapping between ion number and name. For
        networks other than burn.
        """
        mapping = {}
        for i,name in enumerate(self.ions):
            if name != '':
                mapping[name] = i
        
        return mapping
    
    def get_massfraction(self, name, molar=False):
        """
        Return the mass fraction of isotope with given name.
        If the name is not found, 0 is returned.
        Optionally, the molar fraction can be returned.
        """
        if name in self.isotope_mapping:
            ind1 = self.isotope_mapping[name]
            ind2 = n.where(self.ionbmax==name)[0][0]
            if molar:
                return self.ppnb[ind1]
            else:
                return self.ppnb[ind1]*self.nabmax[ind2]
        elif name in self.ion_mapping:
            ind = self.ion_mapping[name]
            if name=='h1':
                # H1 split over two ions: H1, and PN1 from photodisintegration.
                ind1 = self.ion_mapping['pn1']
                return self.ppn[ind] + self.ppn[ind1]
            if molar:
                return self.ppn[ind]
            else:
                return self.ppn[ind]*self.aion[ind]
        else:
            print 'WARNING: isotope', name, 'not found! Returning zeros.'
            return n.zeros_like(self.dm[:-1])

    def get_elemental_massfraction(self, Z):
        """
        Return the mass fractions of element with proton number Z
        """
        isotopes = self.isotope_names()[self.Z()==Z]
        X = n.zeros_like(self.dm)
        for isotope in isotopes:
            X[1:] += self.get_massfraction(isotope)
        
        return X
    
    def get(self, name):
        """Generic get routine to return a variable with a certain
        name. A name may refer to a variable or to a function
        that returns a value.
        Additionally, it may refer to an isotope mass fraction.
        Note: because we have an adaptive network, an isotope may not
        be present if its massfraction is negligible. Therefore, we always
        return a 0-filled array."""
        try:
            return bec.Model.get(self, name)
        except:
            # Add 0 inner grid point to comply with other off-grid properties.
            return n.concatenate([[0], self.get_massfraction(name)])
    
    def cno(self):
        """
        Return total mass fraction of CNO elements.
        """
        total = self.get_massfraction('c12')
        for isotope in ['c13', 'o14', 'o15', 'o16', 'o17', 'n13', 'n14', 'n15']:
            total = total + self.get_massfraction(isotope)
        return n.concatenate([[0], total])

    def isotope_names(self):
        """
        Return an array with the names of all isotopes.
        """
        names = self.create_isotope_mapping().keys()
        if len(names)==0:
            names = self.create_ion_mapping().keys()
        
        return n.array(names)

    def get_A(self, name):
        """
        Number of nuclei per nucleus for isotope with given name.
        """
        try:
            ind2 = n.where(self.ionbmax==name)[0][0]
            return self.nabmax[ind2]
        except:
            return {'he4': 4, 'o16': 16, 'mg24': 24, 'n14': 14, 'he3': 3, 'nt1': 1, 'ne20': 20, 'h1': 1, 's32': 32, 'ca40': 40, 'pn1': 1, 'ar36': 36, 'cr48': 48, 'fe54': 54, 'c12': 12, 'si28': 28, 'ni56': 56, 'fe52': 52, 'ti44': 44}[name]
    
    def A(self):
        """
        Number of nuclei per nucleus for each isotope in the same order as returned
        by isotope_names().
        """
        return n.array([self.get_A(name) for name in self.isotope_names()])
    
    def get_Z(self, name):
        """
        Number of protons per nucleus for isotope with given name.
        """
        try:
            ind2 = n.where(self.ionbmax==name)[0][0]
            return self.nzbmax[ind2]
        except:
            return {'he4': 2, 'o16': 8, 'mg24': 12, 'n14': 7, 'he3': 2, 'nt1': 0, 'ne20': 10, 'h1': 1, 's32': 16, 'ca40': 20, 'pn1': 1, 'ar36': 18, 'cr48': 24, 'fe54': 26, 'c12': 6, 'si28': 14, 'ni56': 28, 'fe52':26, 'ti44': 22}[name]
    
    def Z(self):
        """
        Number of protons per nucleus for each isotope in the same order as returned
        by isotope_names().
        """
        return n.array([self.get_Z(name) for name in self.isotope_names()])

    def gridpoints(self):
        """
        Array of gridpoints (zone numbers). Note that we start at 0 to be
        consistent with output of KEPLER."""
        return n.arange(self.n)

    def cs(self, Gamma=1.0):
        """
        Speed of sound
        """
        return n.sqrt(Gamma*self.pn/self.ro)
    
    def ye(self):
        """
        Number of electrons per baryon.
        """
        return self.zbar/self.abar
    
    def ai(self):
        """
        Moment of inertia
        """
        ri = self.r[:-1]
        ra = self.r[1:]
        dm2 = self.dm[1:]
        rai = ra*ri
        ra2 = ra**2
        ri2 = ri**2
        rm2 = ri2 + rai + ra2
	ai = 0.4*dm2*(ri2**2 + rai*rm2 + ra2**2)/rm2
        return n.concatenate([[1.0], ai])
    
    def hp(self):
        """
        Pressure scale height
        """
        return -1.0*self['pn']/(self['ro']*self['g'])
    
    def vconv(self):
        """
        Convective velocity
        """
        return 3.0*self['diff']/self['hp']
    
    def tconv(self):
        """
        Convective timescale
        """
        return self['hp']**2/(3.0*self['diff'])
    
    def tburn(self):
        """
        Nuclear burning timescale
        """
        return self['cp']*self['t']/self['sn']
    
    def tcool(self):
        """
        Radiative + conductive cooling timescale
        """
        return self['cp']*self['t']/self['ecool']
    
    def tdyn(self):
        """
        Dynamical time scale
        """
        return self['hp']/self['cs']

    def tcond(self):
        """
        Heat conduction timescale. Experimental: correct/useful expression?
        """
        return self['en']*self['dm']/self['sl']

    def dtime(self):
        """
        Provide access to self.p['dtnew'] for kui1.py
        """
        return self.p['dtnew']
    
    def to_ascii(self, filename='', fields=['gridpoints', 'dm', 'ro', 't', 'pn', 'r', 'u', 'sl', 'cap', 'sn'], approx=True,
                 comment='zone number, zone mass, density, temperature, pressure, radius, velocity, luminosity, opacity, specific energy generation rate, isotopic mass fractions'):
        """
        Create an ascii file with filename, containing all the specified fields. If no filename is specified, the name of the dump file is appended
        with '.txt'.
        Optionally all the isotopes in the approx network are added.
        """
        approx_fields = ['nt1', 'h1', 'pn1', 'he3', 'he4', 'c12', 'n14', 'o16', 'ne20', 'mg24', 'si28',
                         's32', 'ar36', 'ca40', 'ti44', 'cr48', 'fe52', 'fe54', 'ni56']
        if approx:
            fields = fields + approx_fields

        if filename=='':
            filename = '%s.txt'%self.filename
        f = open(filename, 'w')

        # Header
        f.write('# %s %s UTC -- created by %s on %s from file %s -- time %.17E\n'%({True: 'APPROX --', False: ''}[approx], time.asctime(time.gmtime()),
                                                                                   os.getenv('USER'), os.getenv('HOSTNAME'), self.filename, self.time))
        f.write('# Mass accretion rate: %.7E Msun/yr\n'%self.p['accrate'])
        f.write('# %s \n'%comment)
        f.write('#\n')
        
        # Column labels
        format = ' '.join(['%25s'%field for field in fields]) + '\n'
        format = '#' + format[1:]
        f.write(format)
        
        # Data
        size = len(self.t)
        data = [self.get(field).copy() for field in fields]
        try:
            data[fields.index('sl')][0] = self.p['xlum0']
        except:
            pass
        try:
            data[fields.index('xln')][0] = self.p['xlum0']
        except:
            pass
        try:
            data[fields.index('dm')][0] = self.p['summ0']
        except:
            pass
        format = ' '.join([{n.dtype('>f8'): '%25.17E', n.dtype('float64'): '%25.17E', n.dtype('int64'): '%25i'}[field.dtype] for field in data]) + '\n'
        for i in xrange(size):
            f.write(format%tuple([field[i] for field in data]))
        
        f.close()

class LcFile:
    """
    Load all information from a KEPLER .lc light curve file.
    """

    def __init__(self, filename, byteorder='>', step=1):
        """
        Load the data from given filename
        """
        self.filename = filename
        self.byteorder = byteorder
        self.step = step
        self.file = bec.FortranFile(filename, byteorder=byteorder)
        self.init_arrays()
        self._int_size = struct.calcsize('i')
        self._double_size = struct.calcsize('d')
        
        self.load(byteorder, step=step)
    
    def init_arrays(self):
        """
        Initialize empty arrays.
        """
        self.nvers = n.array([])
        self.model = n.array([])
        self.time = n.array([])
        self.dt = n.array([])
        self.teff = n.array([])
        self.radius = n.array([])
        self.lum = n.array([])
        self.vphotx = n.array([])
        self.xlphot = n.array([])
        self.radius_profile = []
        self.v_profile = []
        self.lum_profile = []
    
    def load_record(self, data, byteorder='>'):
        """
        Load one record from given data
        """
        fmt = '%si'%byteorder # First read version
        nvers = struct.unpack(fmt, data[:self._int_size])[0]
        
        fmt = {10000: '%siiddddi', # Select format based on version
               10100: '%siidddddi',
               10101: '%siiddddddi',
               10102: '%siidddddddi',
               }
        if nvers in fmt:
            fmt = fmt[nvers]%byteorder
        else:
            raise bec.Model.VersionError(nvers, 10102)
        other_size = struct.calcsize(fmt)
        
        result = struct.unpack(fmt, data[:other_size]) # Read all but profiles
        model = result[1]
        time = result[2]
        if nvers == 10102:
            dt = result[3]
            teff = result[4]
            radius = result[5]
            lum = result[6]
            vphotx = result[7]
            xlphot = result[8]
            jj = result[9] # Number of points in profiles
        elif nvers == 10101:
            dt = result[3]
            teff = result[4]
            radius = result[5]
            lum = result[6]
            vphotx = result[7]
            xlphot = 0.0
            jj = result[8]
        elif nvers == 10100:
            dt = 0.0
            xlphot = 0.0
            teff = result[3]
            radius = result[4]
            lum = result[5]
            vphotx = result[6]
            jj = result[7]
        else:
            dt = 0.0
            xlphot = 0.0
            teff = result[3]
            radius = result[4]
            lum = result[5]
            vphotx = 0.0
            jj = result[6]
        
        mdata = struct.unpack('%s%id'%(byteorder, jj*3), data[other_size:]) # Read profiles
        radius_profile = mdata[::3]
        v_profile = mdata[1::3]
        lum_profile = mdata[2::3]
        
        return {'nvers': nvers,
                'model': model,
                'time': time,
                'dt': dt,
                'teff': teff,
                'radius': radius,
                'lum': lum,
                'vphotx': vphotx,
                'xlphot': xlphot,
                'radius_profile': radius_profile,
                'v_profile': v_profile,
                'lum_profile': lum_profile,
                }
    
    def load(self, byteorder='>', step=1, start=0):
        """
        Load the data
        """
        f = self.file
        
        # Initialize arrays
        nvers = []
        model = []
        time = []
        dt = []
        teff = []
        radius = []
        lum = []
        vphotx = []
        xlphot = []
        radius_profile = []
        v_profile = []
        lum_profile = []
        
        if start%step>0: # Correct start, such that repeated calls always are in phase
            start = start + step - start%step
        for i in xrange(start, len(f), step):
            result = self.load_record(f[i], byteorder=byteorder) # Extract data for one record
            nvers.append(result['nvers'])
            model.append(result['model'])
            time.append(result['time'])
            dt.append(result['dt'])
            teff.append(result['teff'])
            radius.append(result['radius'])
            lum.append(result['lum'])
            vphotx.append(result['vphotx'])
            xlphot.append(result['xlphot'])
            radius_profile.append(result['radius_profile'])
            v_profile.append(result['v_profile'])
            lum_profile.append(result['lum_profile'])
        
        self.nvers = n.concatenate([self.nvers, nvers])
        self.model = n.concatenate([self.model, model])
        self.time = n.concatenate([self.time, time])
        if nvers[-1] < 10101: # No dt saved for these versions
            self.dt = n.concatenate([self.time[1:] - self.time[:-1], [0]])
        else:
            self.dt = n.concatenate([self.dt, dt])
        self.teff = n.concatenate([self.teff, teff])
        self.radius = n.concatenate([self.radius, radius])
        self.lum = n.concatenate([self.lum, lum])
        self.vphotx = n.concatenate([self.vphotx, vphotx])
        self.xlphot = n.concatenate([self.xlphot, xlphot])
        self.radius_profile += radius_profile
        self.v_profile += v_profile
        self.lum_profile += lum_profile
        
        self.t = self.time
    
    def update(self):
        """
        Update the light curve by reading in any new
        data. Changes to older data are not taken into
        account.
        """
        start = len(self.file.index)
        self.file.update_index()
        if len(self.file.index)>start:
            self.load(byteorder=self.byteorder, step=self.step, start=start)
    
    def clean(self, x, number=5, log=False):
        """
        Run a median filter of width number over x.
        If log=True is specified, only positive (>0)
        values are considered.
        """
        x = x.copy()
        if log:
            ind = x>0
        else:
            ind = n.ones(len(x), n.bool)
        x[ind] = signal.medfilt(x[ind], number)
        
        return x
    
    def get_trecurs(self, duration=1e3, level=5):
        """
        See kepler.get_trecurs()
        """
        return get_trecurs(self.time, self.lum, duration, level)
    
    def to_pca(self, distance=8, PCUs=1, NH=0.5e22, mass=2.8e33, fc=1.5, Emin=2, Emax=40, area_file='/home/laurens/software/pimms4_2/data/xte_pca_.area'):
        """
        Convert to pca flux for a source at distance in kpc
        for given number of PCUs. fc is color correction factor: Tc = fc*Teff.
        The energy band is Emin to Emax, rounded to nearest channel boundaries
        within the specified range. Mass is used to calculate gravitational red
        shift.
        Assume an absorbed blackbody with peak at effective temperature from this
        light curve. Takes into account the effective area of the PCA at
        different energies (from pimms).
        """
        # Change to radius and redshift due to GR corrections
        G = bec.Grav
        c = bec.lightspeed
        R = self.radius.min()
        zeta = G*mass/c/c/R
        B = (9*zeta*zeta + n.sqrt(3)*n.sqrt(16 + 27*zeta**4))**(1./3)
        A = (2./9)**(1./3)*(B*B - 2*6**(1./3))/(B*zeta*zeta)
        xi = zeta/2*(1 + n.sqrt(1 - A) + n.sqrt(2 + A + 2./n.sqrt(1 - A)))
        xi2 = xi*xi
        z1 = xi2 # 1 + z1
        
        # Redshift
        #z1 = 1./n.sqrt(1 - 2*G*mass/(c*c*self.radius*n.sqrt(xi))) # 1 + z
        time = self.time*z1
        
        # Energy
        E, A = n.loadtxt(area_file, unpack=True, usecols=(0,1)) # Energy keV, effective area cm2 PCU-1
        ind = n.logical_and(E>=Emin, E<=Emax)
        E = E[ind]
        A = A[ind]
        A = A*PCUs # Effective area for all active PCUs
        K = (1.0/(distance/10.0))**2 # Part of bbodyrad normalization
        kT = fc*self.teff*bec.kb/bec.keV # Color temperature in keV
        R = (self.radius + R*(xi-1))/1e5 # Radius in km
        wabs = models.wabs([NH], E) # Hydrogen absorption at all E
        
        # Rate
        rate = n.zeros_like(self.time)
        factor = 0.5*(E[1:] - E[:-1])*A[:-1]
        for i in xrange(len(rate)):
            t = kT[i]
            k = K*R[i]*R[i]
            #r = models.bbodyrad([t, k], E*z1[i])*wabs # Use blueshifted E
            r = models.bbodyrad([t, k], E*z1)*wabs # Use blueshifted E
            #r = (r[1:] + r[:-1])*(E[1:] - E[:-1])*A[:-1] # For average r, 0.5 is done on next line
            #rate[i] = 0.5*r.sum()
            r = (r[1:] + r[:-1])*factor
            rate[i] = r.sum()
        rate = rate/z1 # Redshift time
        
        return time, rate, kT/z1
    
    def to_jemx(self, distance=8, NH=0.5e22, mass=2.8e33, fc=1.5, Emin=3, Emax=25):
        """
        Convert to JEMX flux. See to_pca() for further information
        """
        return self.to_pca(distance, 1, NH, mass, fc, Emin, Emax, '/home/laurens/software/pimms4_2/data/integral_jemx_.area')
    
    def to_ascii(self, filename, zone=None):
        """
        Export light curve to ascii file with filename. Adding .gz to the filename
        automatically results in compressed output.
        """
        if zone==None:
            lum = self.lum
        else:
            lum = self.lum_from_zone(zone)
        n.savetxt(filename, n.array([self.time, lum]).transpose())
    
    def __str__(self):
        """
        Return a string representation
        """
        return self.filename

    def lum_from_zone(self, zone):
        """
        Return luminosity <zone> zones inward
        """
        return n.array([profile[zone] for profile in self.lum_profile])

    def lum_from_zones(self, zone_start=0, zone_stop=100):
        """
        Return luminosity averaged over lum_profile[zone_start:zone_stop]
        """
        
        return n.array([n.mean(profile[zone_start:zone_stop]) for profile in self.lum_profile])

class ReducedLcFile(LcFile):
    """
    For very large light curve files, this class reduces the resolution of the data while
    reading. Reading may still take a long time, but memory usage is lower.
    """
    
    def __init__(self, filename, byteorder='>', step=1, dx=0.5, dlogy=0.01):
        """
        Load the data from given filename (see LcFile).
        
        Only include data points that differ from the previous point by dx and dlogy.
        """
        self.dx = dx
        self.dlogy = dlogy
        LcFile.__init__(self, filename, byteorder, step)
    
    def load(self, byteorder='>', step=1, start=0):
        """
        Load the data
        """
        f = self.file
        
        # Initialize arrays
        nvers = []
        model = []
        time = []
        dt = []
        teff = []
        radius = []
        lum = []
        vphotx = []
        xlphot = []
        radius_profile = []
        v_profile = []
        lum_profile = []

        dx = self.dx
        dlogy = self.dlogy
        previous = None
        time_since_previous = 0.0
        if start%step>0: # Correct start, such that repeated calls always are in phase
            start = start + step - start%step
        for i in xrange(start, len(f), step):
            result = self.load_record(f[i], byteorder=byteorder) # Extract data for one record
            time_since_previous = time_since_previous + result['dt']
            if previous==None or previous['lum']==0 or (time_since_previous>dx and n.abs(n.log10(result['lum']/previous['lum']))>dlogy):
                previous = result
                time_since_previous = 0.0
                
                nvers.append(result['nvers'])
                model.append(result['model'])
                time.append(result['time'])
                dt.append(result['dt'])
                teff.append(result['teff'])
                radius.append(result['radius'])
                lum.append(result['lum'])
                vphotx.append(result['vphotx'])
                xlphot.append(result['xlphot'])
                radius_profile.append(result['radius_profile'])
                v_profile.append(result['v_profile'])
                lum_profile.append(result['lum_profile'])
        
        self.nvers = n.concatenate([self.nvers, nvers])
        self.model = n.concatenate([self.model, model])
        self.time = n.concatenate([self.time, time])
        if nvers[-1] < 10101: # No dt saved for these versions
            self.dt = n.concatenate([self.time[1:] - self.time[:-1], [0]])
        else:
            self.dt = n.concatenate([self.dt, dt])
        self.teff = n.concatenate([self.teff, teff])
        self.radius = n.concatenate([self.radius, radius])
        self.lum = n.concatenate([self.lum, lum])
        self.vphotx = n.concatenate([self.vphotx, vphotx])
        self.xlphot = n.concatenate([self.xlphot, xlphot])
        self.radius_profile += radius_profile
        self.v_profile += v_profile
        self.lum_profile += lum_profile
        
        self.t = self.time

def load_acc(filename, M=1.4, R=10):
    """
    Load an accretion file. Return:
    - time (s)
    - mass accretion rate (g/s)
    - accretion luminosity (erg/s) for 1.4 Msun, 10 km neutron star
    """
    t, mdot = n.loadtxt(filename, skiprows=2, unpack=True)
    G = 6.673e-8
    lum = mdot*G*(M*1.99e33)/(R*1e5)
    
    return t, mdot, lum
